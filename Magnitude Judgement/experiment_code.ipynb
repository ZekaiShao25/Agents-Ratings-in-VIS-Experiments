{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitude Judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please provide your OPENAI_KEY\n",
    "OPENAI_KEY = \"sk-<your-openai-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run_local_vision_request(text, image_urls, temperature=0):\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    api_key = OPENAI_KEY\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": text},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for image_path in image_urls:\n",
    "        # Getting the base64 string\n",
    "        base64_image = encode_image(image_path)\n",
    "        messages[0][\"content\"].append({\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "        })\n",
    "\n",
    "    headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def run_test(name, description, prompt, image_urls, logger, times=1):\n",
    "    response_list = []\n",
    "    while len(response_list) < times:\n",
    "        try:\n",
    "            response = run_local_vision_request(\n",
    "                text=prompt, \n",
    "                image_urls=image_urls,\n",
    "            )\n",
    "            matched_content = re.search(r'```json([\\s\\S]*?)```', response[\"choices\"][0][\"message\"][\"content\"])\n",
    "            if matched_content:\n",
    "                json_format_data = json.loads(matched_content.group(1))\n",
    "            else: \n",
    "                json_format_data = \"\"\n",
    "            logger.log(name, description, prompt, image_urls, response[\"choices\"][0][\"message\"][\"content\"], json_format_data)\n",
    "            response_list.append(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    return response_list\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        \n",
    "    def log(self, name, description, prompt, image_urls, response, json_format_data):\n",
    "        json_data_to_add = {\"name\": name, \"description\": description, \"prompt\": prompt, \"image_urls\": image_urls, \"response\": response, \"json_format_data\": json_format_data}\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        data.append(json_data_to_add)\n",
    "        with open(self.file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "logger = Logger(\"./magnitude.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_dir = './experiment_material/e1-dot-plots-axes/'\n",
    "list1 = pd.read_csv(f'{material_dir}list1.csv',sep=',',header=0,index_col=['item_no'])\n",
    "list2 = pd.read_csv(f'{material_dir}list2.csv',sep=',',header=0,index_col=['item_no'])\n",
    "\n",
    "item_list = [1, 11, 2, 32, 8, 31, 4, 7, 35, 3, 14, 24, 20, 22, 15, 9, 21, 28, 23, 17]\n",
    "\n",
    "def get_image_urls(index):\n",
    "    if list1['condition'][index] == \"mid\":\n",
    "        return []\n",
    "    list1_img_url = material_dir + list1['img'][index]\n",
    "    list2_img_url = material_dir + list2['img'][index]\n",
    "    if list1['condition'][index] == 'hi':\n",
    "        return [list1_img_url, list2_img_url]\n",
    "    else: \n",
    "        return [list2_img_url, list1_img_url]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE = \"You are an average user. \"\n",
    "def TASK(index):\n",
    "    return list1['scenario'][index]+'\\n'+list1['graph_cat'][index]+'\\n'+'Task: '+'\\n'+list1['if_phrase'][index] +'\\n'+list1['chance_cat'][index]+' Answer on a scale from 1(very unlikely) to 7(very likely).' + '\\n' + list1['severity_cat'][index]+' Answer on a scale from 1(very mild) to 7(very severe).' + \\\n",
    "        \"For each question, please rate each graph with only one score.\" + \\\n",
    "        'Rate the following two graphs separately and give the reason.'\n",
    "JSON_FORMAT = \"Please give an additional scoring result in json format at the end of your answe, like ```json{{'chance': [score(for graph1), score(for graph2)], 'severity': [score(for graph1), score(for graph2)] }}```.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"magnitude_experience1\"\n",
    "for item_no in item_list:\n",
    "    description = item_no\n",
    "    prompt = TASK(item_no) + JSON_FORMAT\n",
    "    image_urls = get_image_urls(item_no)\n",
    "    if len(image_urls) > 0:\n",
    "        run_test(name, description, prompt, image_urls, logger, times=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_dir = './experiment_material/e2-dot-plots-axes/'\n",
    "list1 = pd.read_csv(f'{material_dir}list1.csv',sep=',',header=0,index_col=['item_no'])\n",
    "list2 = pd.read_csv(f'{material_dir}list2.csv',sep=',',header=0,index_col=['item_no'])\n",
    "\n",
    "item_list = [1, 11, 2, 32, 8, 31, 4, 7, 35, 3, 14, 24, 20, 22, 15, 9, 21, 28, 23, 17]\n",
    "\n",
    "def get_image_urls(index):\n",
    "    if list1['condition'][index] == \"mid\":\n",
    "        return []\n",
    "    list1_img_url = material_dir + list1['img'][index]\n",
    "    list2_img_url = material_dir + list2['img'][index]\n",
    "    if list1['condition'][index] == 'hi':\n",
    "        return [list1_img_url, list2_img_url]\n",
    "    else: \n",
    "        return [list2_img_url, list1_img_url]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE = \"You are an average user. \"\n",
    "def TASK(index):\n",
    "    return list1['scenario'][index]+'\\n'+list1['graph_cat'][index]+'\\n'+'Task: '+'\\n'+list1['if_phrase'][index] +'\\n'+list1['chance_cat'][index]+' Answer on a scale from 1(very unlikely) to 7(very likely).' + '\\n' + list1['severity_cat'][index]+' Answer on a scale from 1(very mild) to 7(very severe).' + \\\n",
    "        \"For each question, please rate each graph with only one score.\" + \\\n",
    "        'Notice that he arrow on the \"Chance\" axis points downwards, meaning the numbers get bigger as the axis goes down.' + \\\n",
    "        'Rate the following two graphs separately and give the reason.'\n",
    "JSON_FORMAT = \"Please give an additional scoring result in json format at the end of your answe, like ```json{{'chance': [score(for graph1), score(for graph2)], 'severity': [score(for graph1), score(for graph2)] }}```.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"magnitude_experience2\"\n",
    "for item_no in item_list:\n",
    "    description = item_no\n",
    "    prompt = TASK(item_no) + JSON_FORMAT\n",
    "    image_urls = get_image_urls(item_no)\n",
    "    if len(image_urls) > 0:\n",
    "        run_test(name, description, prompt, image_urls, logger, times=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
